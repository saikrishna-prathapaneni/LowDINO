{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install timm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJ0MVgB2awPd",
        "outputId": "67552801-417d-4c91-edf6-a802a78bf03b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting timm\n",
            "  Downloading timm-0.9.2-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from timm) (2.0.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.15.1+cu118)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0)\n",
            "Collecting huggingface-hub (from timm)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors (from timm)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm) (16.0.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2023.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (23.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.22.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7->timm) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->timm) (1.3.0)\n",
            "Installing collected packages: safetensors, huggingface-hub, timm\n",
            "Successfully installed huggingface-hub-0.14.1 safetensors-0.3.1 timm-0.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import timm\n",
        "from torch.nn import functional as F\n",
        "from torchsummary import summary\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import numpy as np\n",
        "# from model import Head, MultiCrop,DinoLoss\n",
        "# from Augmentation import DataAugmentation\n",
        "# from PIL import ImagePath\n",
        "# from torchvision.datasets import ImageFolder\n",
        "# import pathlib\n",
        "# from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "# model and dim values\n",
        "\n",
        "\n",
        "mobile_models = {\n",
        "    'mobilevit_s':640,\n",
        "    'mobilevit_xs':640,\n",
        "    'mobilevit_xxs':640,\n",
        "    'mobilenetv2_035':640,\n",
        "    'mobilenetv2_075':640,\n",
        "    'mobilenetv2_100':640,\n",
        "    'resnet5m':512,   \n",
        "}\n",
        "\n",
        "class mobilenet(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model:str = 'mobilenetv2_100',\n",
        "                 pretrained=True):\n",
        "        super(mobilenet,self).__init__()\n",
        "        self.backbone = timm.create_model(model,pretrained=pretrained)\n",
        "        self.backbone.reset_classifier(0)\n",
        "        self.num_features = self.backbone.num_features\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.backbone(x)\n",
        "        return x\n",
        "\n",
        "class MultiCrop(nn.Module):\n",
        "    \"\"\"\n",
        "    backbone: timm.models.vision_transformer.VisionTransformer\n",
        "    new_head: head\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 backbone,\n",
        "                 new_head,\n",
        "                 mobile_head=False\n",
        "                 ) -> None:\n",
        "        super().__init__()\n",
        "        self.mobile_head =mobile_head \n",
        "\n",
        "        #setting up the model\n",
        "        self.backbone = backbone\n",
        "        backbone.head= nn.Identity()\n",
        "        self.new_head = new_head\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        \"\"\"\n",
        "        x is List of torch.Tensor of shape (n_samples, 3,size,size)\n",
        "        \n",
        "        \"\"\"\n",
        "        n_crops = len(x)\n",
        "        #print(\"len of batch \",len(x))\n",
        "        concatenated_tensor = torch.cat(x,dim=0) \n",
        "        # (n_samples*n_crops, 3, size, size)\n",
        "        # example batch size of 64 we have [640,3, 224,224] for size crops of 10: 2G,8L\n",
        "        \n",
        "        #print(\"shape of concat tensor\",concatenated_tensor.shape)\n",
        "        cls_embedding = self.backbone(concatenated_tensor) # (n_samples * n_crops, in_dim)\n",
        "        #print(cls_embedding.shape, \"cls embedding\")\n",
        "        logits =self.new_head(cls_embedding) # n_samples * n_crops, out_dim\n",
        "\n",
        "        chunks = logits.chunk(n_crops) # n_crops * (n_samples,outdim)\n",
        "        \n",
        "        return chunks\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_dim,\n",
        "                 out_dim,\n",
        "                 hidden_dim = 512,\n",
        "                 bottleneck_dim = 256,\n",
        "                 n_layers =3,\n",
        "                 norm_last_layer=False,\n",
        "                 init_weights=[\"normal\",\"\"] # yet to define\n",
        "                 ) -> None:\n",
        "        super().__init__()\n",
        "        \n",
        "        # create a Multilayer perceptron based on the layer number from in dim to out dim\n",
        "       \n",
        "        if n_layers ==1:\n",
        "            self.mlp =nn.Linear(in_dim, bottleneck_dim)\n",
        "        else:\n",
        "            layers = [nn.Linear(in_dim, hidden_dim)]\n",
        "            layers.append(nn.SELU())\n",
        "            for _ in range(n_layers-2):\n",
        "                layers.append(nn.Linear(hidden_dim,hidden_dim))\n",
        "                layers.append(nn.SELU())\n",
        "            layers.append(nn.Linear(hidden_dim,bottleneck_dim))\n",
        "            self.mlp = nn.Sequential(*layers)\n",
        "        \n",
        "        \n",
        "        self.apply(self._init_weights)\n",
        "        self.last_layer = nn.utils.weight_norm(\n",
        "            nn.Linear(bottleneck_dim,out_dim,bias=False)\n",
        "        )\n",
        "        self.last_layer.weight_g.data.fill_(1)\n",
        "        if norm_last_layer:\n",
        "            self.last_layer.weight_g.requires_grad=False\n",
        "        \n",
        "    def _init_weights(self,m):\n",
        "        if isinstance(m,nn.Linear):\n",
        "            nn.init.normal_(m.weight,std=0.02)\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias,0)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x= self.mlp(x)\n",
        "        x= F.normalize(x,dim=-1,p=2)\n",
        "        x=self.last_layer(x)\n",
        "        return x\n",
        "    \n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "import timm\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self,\n",
        "                inchannels,\n",
        "                outchannels,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                skip=True):\n",
        "        super().__init__()\n",
        "        # Determines whether to add the identity mapping skip connection\n",
        "        self.skip = skip\n",
        "        \n",
        "        # First block of the residual connection\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(inchannels,\n",
        "                    outchannels,\n",
        "                    kernel_size=kernel_size,\n",
        "                    stride=stride,\n",
        "                    padding=1,\n",
        "                    bias=False),\n",
        "            nn.BatchNorm2d(outchannels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(outchannels,\n",
        "                    outchannels,\n",
        "                    kernel_size=kernel_size,\n",
        "                    padding=1,\n",
        "                    bias=False),\n",
        "            nn.BatchNorm2d(outchannels),\n",
        "        )\n",
        "        \n",
        "        # If the stride is 2 or input channels and output channels do not match,\n",
        "        # then add a convolutional layer and a batch normalization layer to the identity mapping\n",
        "        if stride == 2 or inchannels != outchannels:\n",
        "            self.skip = False\n",
        "            self.skip_conv = nn.Conv2d(inchannels, outchannels, kernel_size=1, stride=stride, bias=False)\n",
        "            self.skip_bn = nn.BatchNorm2d(outchannels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.block(x)\n",
        "        \n",
        "        # If the skip connection is active, add the input to the output\n",
        "        # If the skip connection is not active, add the skip connection to the output\n",
        "        if not self.skip:\n",
        "            out += self.skip_bn(self.skip_conv(x))\n",
        "        else:\n",
        "            out += x\n",
        "        \n",
        "        out = F.relu(out.clone())\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet5M(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Initial convolutional layer and batch normalization\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
        "        \n",
        "        # Residual blocks\n",
        "        self.resblock3 = ResBlock(64, 64, stride=1)\n",
        "        self.resblock6 = ResBlock(64, 64, stride=1)\n",
        "        self.resblock7 = ResBlock(64, 64, stride=1)\n",
        "        self.resblock8 = ResBlock(64, 128, stride=2)\n",
        "        self.resblock9 = ResBlock(128, 128, stride=1)\n",
        "        self.resblock10 = ResBlock(128, 128, stride=1)\n",
        "        self.resblock11 = ResBlock(128, 128, stride=1)\n",
        "        self.resblock12 = ResBlock(128, 128, stride=1)\n",
        "        self.resblock13 = ResBlock(128, 128, stride=1)\n",
        "        self.resblock14 = ResBlock(128, 512, stride=2)\n",
        "        \n",
        "        # Global average pooling and fully-connected layer\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
        "        self.flat = nn.Flatten()\n",
        "        # self.fc = nn.Linear(in_features=512, out_features=10, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x.clone())\n",
        "        x = self.maxpool(x)\n",
        "        x = self.resblock3(x)\n",
        "        x = self.resblock6(x)\n",
        "        x = self.resblock7(x)\n",
        "        x = self.resblock8(x)\n",
        "        x = self.resblock9(x)\n",
        "        x = self.resblock10(x)\n",
        "        x = self.resblock11(x)\n",
        "        x = self.resblock12(x)\n",
        "        x = self.resblock13(x)\n",
        "        x = self.resblock14(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = self.flat(x)\n",
        "        # x = self.fc(x) \n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "esKVHfYyaqNq"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "gp6HLJlGYgA8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80e5d76a-8a37-4e08-c797-6f4b1090e704"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1280])"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ],
      "source": [
        "# student_vit= timm.create_model('mobilevit_s', pretrained=False).to('cuda')\n",
        "\n",
        "student_vit = mobilenet()\n",
        "# student_vit=ResNet5M()\n",
        "# student_vit=torch.hub.load('facebookresearch/dino:main', 'dino_resnet50').to('cuda')\n",
        "# student_vit=torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14').to('cuda')\n",
        "# student_vit.reset_classifier(0)\n",
        "model = MultiCrop(\n",
        "        student_vit,\n",
        "        Head(\n",
        "            # 512,  # for resnet 5m\n",
        "             640,  # for all other models\n",
        "            1024\n",
        "        ),\n",
        "    )\n",
        "model=nn.DataParallel(model).to('cuda')\n",
        "\n",
        "\n",
        "# model = timm.create_model('mobilevit_s', pretrained=True).to('cuda')\n",
        "# model = timm.create_model('mobilenetv2_100', pretrained=True).to('cuda')\n",
        "\n",
        "\n",
        "# checkpoint = torch.load('/content/resnet5m_student_model_epoch32.pth')\n",
        "# checkpoint = torch.load('/content/mobile_vit_s_student_model_epoch99.pth')\n",
        "# model.load_state_dict(checkpoint['model_state_dict'])\n",
        "m=torch.randn(1,3,224,224).to('cuda')\n",
        "# with torch.no_grad():\n",
        "#     o1 = model.module.backbone(m)\n",
        "# model = model.module.backbone()\n",
        "model.module.backbone(m).shape\n",
        "# print(model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torch.utils.data as data\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    \n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize((56,56)),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) \n",
        "])\n",
        "\n",
        "cifar_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "tr_split_len =5000 # 10% split data\n",
        "# tr_split_len =15000 # 30% split data\n",
        "cifar_dataset = torch.utils.data.random_split(cifar_dataset, [tr_split_len,len(cifar_dataset)-tr_split_len])[0]\n",
        "\n",
        "batch_size = 32\n",
        "train_dataloader = data.DataLoader(cifar_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "Vq-nw-A6Irnl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bdca91a-149f-42c2-aa57-d5946eb4244a"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "num_samples = len(cifar_dataset)\n",
        "# embedding_size = 512  # for resnet 5m\n",
        "embedding_size = 1280  # mobile_netv2_100\n",
        "# embedding_size = 2048   # for dino v1\n",
        "# embedding_size = 384   # for dino v2\n",
        "# embedding_size = 640   # for all other models\n",
        "train_embeddings = np.zeros((num_samples, embedding_size))\n",
        "test_embeddings = np.zeros((len(test_dataset), embedding_size))\n",
        "train_labels=np.zeros((num_samples))\n",
        "test_labels=np.zeros((len(test_dataset)))\n",
        "\n"
      ],
      "metadata": {
        "id": "oBBwsEx2byEj"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "model=model.to('cuda')\n",
        "with torch.no_grad():\n",
        "    image_idx = 0\n",
        "    for images, labels in train_dataloader:\n",
        "        batch_size = images.size(0)\n",
        "        images = images.to('cuda')  \n",
        "        \n",
        "        outputs = model.module.backbone(images)\n",
        "        \n",
        "        train_embeddings[image_idx:image_idx+batch_size] = outputs.squeeze().cpu().numpy()\n",
        "        train_labels[image_idx:image_idx+batch_size] = labels.squeeze().cpu().numpy()\n",
        "\n",
        "        \n",
        "        image_idx += batch_size\n",
        "\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    image_idx = 0\n",
        "    for images, labels in test_loader:\n",
        "        batch_size = images.size(0)\n",
        "        images = images.to('cuda')  \n",
        "        \n",
        "        outputs = model.module.backbone(images)\n",
        "        \n",
        "        test_embeddings[image_idx:image_idx+batch_size] = outputs.squeeze().cpu().numpy()\n",
        "        test_labels[image_idx:image_idx+batch_size] = labels.squeeze().cpu().numpy()\n",
        "\n",
        "        \n",
        "        image_idx += batch_size\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9jgUQ9Ehb1Nz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92070302-7c35-422a-8196-869abce35aae"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_embeddings.shape)\n",
        "print(test_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yb_VUBRnt1IB",
        "outputId": "7b0edad5-800a-4769-9347-5ccde5087d41"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5000, 1280)\n",
            "(10000, 1280)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "# model.add(Dense(64, input_dim=512, activation='relu')) # for resnet 5m\n",
        "model.add(Dense(64, input_dim=1280, activation='relu')) # for mobile_netv2_100\n",
        "# model.add(Dense(512, input_dim=2048, activation='relu')) # for dino v1\n",
        "# model.add(Dense(512, input_dim=384, activation='relu')) # for dino v2\n",
        "# model.add(Dense(512, input_dim=640, activation='relu')) # for all other models\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "# Convert labels to one-hot encoding if necessary\n",
        "# Assuming label_array contains class indices (0 to 9) for each image\n",
        "from keras.utils import to_categorical\n",
        "labels = to_categorical(train_labels, num_classes=10)\n",
        "# Train the model\n",
        "model.fit(train_embeddings, labels, epochs=10, batch_size=32)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3yu2_uFbbwy",
        "outputId": "f8e48e7c-1606-476e-e228-199e7a9a6a15"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "157/157 [==============================] - 1s 3ms/step - loss: 1.5416 - accuracy: 0.4570\n",
            "Epoch 2/10\n",
            "157/157 [==============================] - 0s 3ms/step - loss: 1.0151 - accuracy: 0.6424\n",
            "Epoch 3/10\n",
            "157/157 [==============================] - 0s 3ms/step - loss: 0.7807 - accuracy: 0.7264\n",
            "Epoch 4/10\n",
            "157/157 [==============================] - 0s 3ms/step - loss: 0.5964 - accuracy: 0.8034\n",
            "Epoch 5/10\n",
            "157/157 [==============================] - 0s 3ms/step - loss: 0.4527 - accuracy: 0.8514\n",
            "Epoch 6/10\n",
            "157/157 [==============================] - 0s 3ms/step - loss: 0.3229 - accuracy: 0.9098\n",
            "Epoch 7/10\n",
            "157/157 [==============================] - 0s 3ms/step - loss: 0.2401 - accuracy: 0.9400\n",
            "Epoch 8/10\n",
            "157/157 [==============================] - 0s 3ms/step - loss: 0.1569 - accuracy: 0.9746\n",
            "Epoch 9/10\n",
            "157/157 [==============================] - 0s 3ms/step - loss: 0.1073 - accuracy: 0.9868\n",
            "Epoch 10/10\n",
            "157/157 [==============================] - 0s 3ms/step - loss: 0.0707 - accuracy: 0.9950\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd82249a860>"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "linear_preds=model.predict(test_embeddings)\n",
        "predicted_labels = np.argmax(linear_preds, axis=1)\n",
        "accuracy_score(predicted_labels,test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWALvX2jbngB",
        "outputId": "e64bbe5f-27a6-42df-ab74-6d0ca0672da3"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.567"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IU6IUGwxoUzO"
      },
      "execution_count": 97,
      "outputs": []
    }
  ]
}