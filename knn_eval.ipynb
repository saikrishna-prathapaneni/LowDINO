{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install timm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2SuukTawFXp",
        "outputId": "3855c7f6-bfa3-49e9-8a1b-5b381ecbbd2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (0.9.2)\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from timm) (2.0.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.15.1+cu118)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.14.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm) (16.0.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2023.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (23.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.22.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7->timm) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->timm) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjuVWMN6v0hN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import timm\n",
        "from torch.nn import functional as F\n",
        "from torchsummary import summary\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import numpy as np\n",
        "# from model import Head, MultiCrop,DinoLoss\n",
        "# from Augmentation import DataAugmentation\n",
        "# from PIL import ImagePath\n",
        "# from torchvision.datasets import ImageFolder\n",
        "# import pathlib\n",
        "# from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "# model and dim values\n",
        "\n",
        "\n",
        "mobile_models = {\n",
        "    'mobilevit_s':640,\n",
        "    'mobilevit_xs':640,\n",
        "    'mobilevit_xxs':640,\n",
        "    'mobilenetv2_035':640,\n",
        "    'mobilenetv2_075':640,\n",
        "    'mobilenetv2_100':640,\n",
        "    'resnet5m':512,   \n",
        "}\n",
        "\n",
        "class mobilenet(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model:str = 'mobilevit_s',\n",
        "                 pretrained=True):\n",
        "        super(mobilenet,self).__init__()\n",
        "        self.backbone = timm.create_model(model,pretrained=pretrained)\n",
        "        self.backbone.reset_classifier(0)\n",
        "        self.num_features = self.backbone.num_features\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.backbone(x)\n",
        "        return x\n",
        "\n",
        "class MultiCrop(nn.Module):\n",
        "    \"\"\"\n",
        "    backbone: timm.models.vision_transformer.VisionTransformer\n",
        "    new_head: head\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 backbone,\n",
        "                 new_head,\n",
        "                 mobile_head=False\n",
        "                 ) -> None:\n",
        "        super().__init__()\n",
        "        self.mobile_head =mobile_head \n",
        "\n",
        "        #setting up the model\n",
        "        self.backbone = backbone\n",
        "        backbone.head= nn.Identity()\n",
        "        self.new_head = new_head\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        \"\"\"\n",
        "        x is List of torch.Tensor of shape (n_samples, 3,size,size)\n",
        "        \n",
        "        \"\"\"\n",
        "        n_crops = len(x)\n",
        "        #print(\"len of batch \",len(x))\n",
        "        concatenated_tensor = torch.cat(x,dim=0) \n",
        "        # (n_samples*n_crops, 3, size, size)\n",
        "        # example batch size of 64 we have [640,3, 224,224] for size crops of 10: 2G,8L\n",
        "        \n",
        "        #print(\"shape of concat tensor\",concatenated_tensor.shape)\n",
        "        cls_embedding = self.backbone(concatenated_tensor) # (n_samples * n_crops, in_dim)\n",
        "        #print(cls_embedding.shape, \"cls embedding\")\n",
        "        logits =self.new_head(cls_embedding) # n_samples * n_crops, out_dim\n",
        "\n",
        "        chunks = logits.chunk(n_crops) # n_crops * (n_samples,outdim)\n",
        "        \n",
        "        return chunks\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_dim,\n",
        "                 out_dim,\n",
        "                 hidden_dim = 512,\n",
        "                 bottleneck_dim = 256,\n",
        "                 n_layers =3,\n",
        "                 norm_last_layer=False,\n",
        "                 init_weights=[\"normal\",\"\"] # yet to define\n",
        "                 ) -> None:\n",
        "        super().__init__()\n",
        "        \n",
        "        # create a Multilayer perceptron based on the layer number from in dim to out dim\n",
        "       \n",
        "        if n_layers ==1:\n",
        "            self.mlp =nn.Linear(in_dim, bottleneck_dim)\n",
        "        else:\n",
        "            layers = [nn.Linear(in_dim, hidden_dim)]\n",
        "            layers.append(nn.SELU())\n",
        "            for _ in range(n_layers-2):\n",
        "                layers.append(nn.Linear(hidden_dim,hidden_dim))\n",
        "                layers.append(nn.SELU())\n",
        "            layers.append(nn.Linear(hidden_dim,bottleneck_dim))\n",
        "            self.mlp = nn.Sequential(*layers)\n",
        "        \n",
        "        \n",
        "        self.apply(self._init_weights)\n",
        "        self.last_layer = nn.utils.weight_norm(\n",
        "            nn.Linear(bottleneck_dim,out_dim,bias=False)\n",
        "        )\n",
        "        self.last_layer.weight_g.data.fill_(1)\n",
        "        if norm_last_layer:\n",
        "            self.last_layer.weight_g.requires_grad=False\n",
        "        \n",
        "    def _init_weights(self,m):\n",
        "        if isinstance(m,nn.Linear):\n",
        "            nn.init.normal_(m.weight,std=0.02)\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias,0)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x= self.mlp(x)\n",
        "        x= F.normalize(x,dim=-1,p=2)\n",
        "        x=self.last_layer(x)\n",
        "        return x\n",
        "    \n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "import timm\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self,\n",
        "                inchannels,\n",
        "                outchannels,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                skip=True):\n",
        "        super().__init__()\n",
        "        # Determines whether to add the identity mapping skip connection\n",
        "        self.skip = skip\n",
        "        \n",
        "        # First block of the residual connection\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(inchannels,\n",
        "                    outchannels,\n",
        "                    kernel_size=kernel_size,\n",
        "                    stride=stride,\n",
        "                    padding=1,\n",
        "                    bias=False),\n",
        "            nn.BatchNorm2d(outchannels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(outchannels,\n",
        "                    outchannels,\n",
        "                    kernel_size=kernel_size,\n",
        "                    padding=1,\n",
        "                    bias=False),\n",
        "            nn.BatchNorm2d(outchannels),\n",
        "        )\n",
        "        \n",
        "        # If the stride is 2 or input channels and output channels do not match,\n",
        "        # then add a convolutional layer and a batch normalization layer to the identity mapping\n",
        "        if stride == 2 or inchannels != outchannels:\n",
        "            self.skip = False\n",
        "            self.skip_conv = nn.Conv2d(inchannels, outchannels, kernel_size=1, stride=stride, bias=False)\n",
        "            self.skip_bn = nn.BatchNorm2d(outchannels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.block(x)\n",
        "        \n",
        "        # If the skip connection is active, add the input to the output\n",
        "        # If the skip connection is not active, add the skip connection to the output\n",
        "        if not self.skip:\n",
        "            out += self.skip_bn(self.skip_conv(x))\n",
        "        else:\n",
        "            out += x\n",
        "        \n",
        "        out = F.relu(out.clone())\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet5M(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Initial convolutional layer and batch normalization\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
        "        \n",
        "        # Residual blocks\n",
        "        self.resblock3 = ResBlock(64, 64, stride=1)\n",
        "        self.resblock6 = ResBlock(64, 64, stride=1)\n",
        "        self.resblock7 = ResBlock(64, 64, stride=1)\n",
        "        self.resblock8 = ResBlock(64, 128, stride=2)\n",
        "        self.resblock9 = ResBlock(128, 128, stride=1)\n",
        "        self.resblock10 = ResBlock(128, 128, stride=1)\n",
        "        self.resblock11 = ResBlock(128, 128, stride=1)\n",
        "        self.resblock12 = ResBlock(128, 128, stride=1)\n",
        "        self.resblock13 = ResBlock(128, 128, stride=1)\n",
        "        self.resblock14 = ResBlock(128, 512, stride=2)\n",
        "        \n",
        "        # Global average pooling and fully-connected layer\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
        "        self.flat = nn.Flatten()\n",
        "        # self.fc = nn.Linear(in_features=512, out_features=10, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x.clone())\n",
        "        x = self.maxpool(x)\n",
        "        x = self.resblock3(x)\n",
        "        x = self.resblock6(x)\n",
        "        x = self.resblock7(x)\n",
        "        x = self.resblock8(x)\n",
        "        x = self.resblock9(x)\n",
        "        x = self.resblock10(x)\n",
        "        x = self.resblock11(x)\n",
        "        x = self.resblock12(x)\n",
        "        x = self.resblock13(x)\n",
        "        x = self.resblock14(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = self.flat(x)\n",
        "        # x = self.fc(x) \n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model = timm.create_model('mobilevit_s', pretrained=True).to('cuda')\n",
        "\n",
        "# model.reset_classifier(0)\n",
        "\n",
        "# model=torch.hub.load('facebookresearch/dino:main', 'dino_resnet50').to('cuda')\n",
        "# model=torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14').to('cuda')\n",
        "\n",
        "# print(model)"
      ],
      "metadata": {
        "id": "EHjufAyumDMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# student_vit=mobilenet()\n",
        "student_vit=ResNet5M()\n",
        "model = MultiCrop(\n",
        "        student_vit,\n",
        "        Head(\n",
        "            512,\n",
        "            1024\n",
        "        ),\n",
        "    )\n",
        "model=nn.DataParallel(model)\n",
        "# model = timm.create_model('mobilevit_s', pretrained=True)\n",
        "\n",
        "\n",
        "# # torch.save(model.state_dict(),'test.pth')\n",
        "m=torch.randn(1,3,224,224).to('cuda')\n",
        "with torch.no_grad():\n",
        "    o1 = model.module.backbone(m)\n",
        "checkpoint = torch.load('./resnet5m_student_model_epoch32.pth')\n",
        "\n",
        "\n",
        "\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "# o2=model.backbone(m)\n",
        "\n",
        "# model=model.to('cuda')\n",
        "o1.shape"
      ],
      "metadata": {
        "id": "Y9zOMx0oxnI7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9d107d5-853b-4dd3-ced7-54371827357b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    \n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize((56,56)),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) \n",
        "])\n",
        "\n",
        "cifar_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)"
      ],
      "metadata": {
        "id": "SuzzSE8o381u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9da9968d-b8bd-4276-dc18-4ac87a126962"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:05<00:00, 29844033.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.utils.data as data\n",
        "\n",
        "batch_size = 32\n",
        "dataloader = data.DataLoader(cifar_dataset, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "Jprgitxz4Add"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "num_samples = len(cifar_dataset)\n",
        "embedding_size = 512\n",
        "embeddings = np.zeros((num_samples, embedding_size))"
      ],
      "metadata": {
        "id": "9pMYwCEw4SfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "model=model.to('cuda')\n",
        "with torch.no_grad():\n",
        "    image_idx = 0\n",
        "    for images, _ in dataloader:\n",
        "        batch_size = images.size(0)\n",
        "        images = images.to('cuda')  \n",
        "        \n",
        "        outputs = model.module.backbone(images)\n",
        "        \n",
        "        embeddings[image_idx:image_idx+batch_size] = outputs.squeeze().cpu().numpy()\n",
        "        \n",
        "        image_idx += batch_size\n"
      ],
      "metadata": {
        "id": "D_sEJJ2b5RUH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "027421e5-838e-4613-a91e-08f04a325dd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings.shape\n",
        "np.save(\"embeddings.npy\", embeddings)"
      ],
      "metadata": {
        "id": "Biz8w2rq5ykb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ys5bUNpxv0hP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95e9b4a9-3b71-4d42-dd6f-569f3936b5df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "k = 5\n",
        "knn = NearestNeighbors(n_neighbors=k)\n",
        "knn.fit(embeddings)\n",
        "\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "\n",
        "output_tensor = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, _ in test_loader:\n",
        "        images = images.to('cuda') \n",
        "        output = model.module.backbone(images)\n",
        "        output_tensor.append(output.squeeze().cpu().numpy())\n",
        "\n",
        "output_tensor = np.stack(output_tensor)\n",
        "_, indices = knn.kneighbors(output_tensor)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_array = cifar_dataset.targets\n",
        "true_test_labels=test_dataset.targets"
      ],
      "metadata": {
        "id": "6MGNlEeyCe_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels = []\n",
        "for i in range(len(indices)):\n",
        "    train_indices = indices[i]\n",
        "    first_train_label = label_array[train_indices[0]]\n",
        "    test_labels.append(first_train_label)\n"
      ],
      "metadata": {
        "id": "j3meFL-t_C3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy_score(true_test_labels,test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNLnRsKi_MLl",
        "outputId": "aa45f995-e97a-4d9e-f1fb-c590033091f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3433"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "model.add(Dense(512, input_dim=640, activation='relu'))\n",
        "model.add(Dense(100, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "# Convert labels to one-hot encoding if necessary\n",
        "# Assuming label_array contains class indices (0 to 9) for each image\n",
        "from keras.utils import to_categorical\n",
        "labels = to_categorical(label_array, num_classes=100)\n",
        "\n",
        "# Train the model\n",
        "model.fit(embeddings, labels, epochs=20, batch_size=32)\n"
      ],
      "metadata": {
        "id": "iS-gqnfPEgR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "linear_preds=model.predict(output_tensor)\n",
        "predicted_labels = np.argmax(linear_preds, axis=1)"
      ],
      "metadata": {
        "id": "fIF75zOuI0ME",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0f32033-2bb7-4010-f058-26f17e966890"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 0s 1ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_labels"
      ],
      "metadata": {
        "id": "suRXy7NXLfJ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41c72d02-2fcd-4bc1-a560-aa3462314229"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 8, 8, ..., 5, 1, 7])"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(predicted_labels,true_test_labels)"
      ],
      "metadata": {
        "id": "AaddFUveQbT8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c25eccee-38d7-4484-ddfd-5440fb311508"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7034"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hh9vVjfkSeCb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cuda",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}